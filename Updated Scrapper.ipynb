{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362f3286-7394-498a-9370-528dd2cdc767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\guppa\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\guppa\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\guppa\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\guppa\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\guppa\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/624.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/624.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 20.5/624.3 kB 93.9 kB/s eta 0:00:07\n",
      "   --- ----------------------------------- 61.4/624.3 kB 328.2 kB/s eta 0:00:02\n",
      "   ---- ---------------------------------- 71.7/624.3 kB 302.7 kB/s eta 0:00:02\n",
      "   ---- ---------------------------------- 71.7/624.3 kB 302.7 kB/s eta 0:00:02\n",
      "   ---- ---------------------------------- 71.7/624.3 kB 302.7 kB/s eta 0:00:02\n",
      "   ---- ---------------------------------- 71.7/624.3 kB 302.7 kB/s eta 0:00:02\n",
      "   ------------ ------------------------- 204.8/624.3 kB 541.9 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 235.5/624.3 kB 515.5 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 317.4/624.3 kB 656.4 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 440.3/624.3 kB 809.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 563.2/624.3 kB 956.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 583.7/624.3 kB 992.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 624.3/624.3 kB 959.4 kB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.5 MB 5.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.3/1.5 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.6/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "Successfully installed nltk-3.9.1 textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c22cca0c-a11b-430d-b434-335de93f6613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Guppa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Guppa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Guppa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Guppa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Guppa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Guppa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef2596a-1c1d-4d35-a083-26f6b5a8d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading comments...\n",
      " Loaded 912467 comments.\n",
      " Vectorizing comments...\n",
      " Performing KMeans clustering...\n",
      " Extracting top words per cluster...\n",
      " Collecting sample comments...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Words Per Cluster</th>\n",
       "      <th>Sample Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bitcoin, cash, https, people, just, buy, like,...</td>\n",
       "      <td>This subreddit is not a place where companies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>way, buy, lol, think, like, sell, good, better...</td>\n",
       "      <td>You don’t. You HODL. BTC is worth way more.\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just, like, crypto, people, lol, buy, good, mo...</td>\n",
       "      <td>----------------------------------\\n\\n**Some f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>don, know, thanks, people, just, think, like, ...</td>\n",
       "      <td>Once in a lifetime, you hear a story. You just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deleted, removed, post, comment, did, account,...</td>\n",
       "      <td>[removed]\\n\\n[removed]\\n\\n[removed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Top Words Per Cluster  \\\n",
       "0  bitcoin, cash, https, people, just, buy, like,...   \n",
       "1  way, buy, lol, think, like, sell, good, better...   \n",
       "2  just, like, crypto, people, lol, buy, good, mo...   \n",
       "3  don, know, thanks, people, just, think, like, ...   \n",
       "4  deleted, removed, post, comment, did, account,...   \n",
       "\n",
       "                                     Sample Comments  \n",
       "0  This subreddit is not a place where companies ...  \n",
       "1  You don’t. You HODL. BTC is worth way more.\\n\\...  \n",
       "2  ----------------------------------\\n\\n**Some f...  \n",
       "3  Once in a lifetime, you hear a story. You just...  \n",
       "4                [removed]\\n\\n[removed]\\n\\n[removed]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scraping r/CryptoCurrency...\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_1.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_2.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_3.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_4.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_5.json\n",
      " Sleeping 6.16s to avoid rate limit...\n",
      " Scraping r/CryptoMarkets...\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_6.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_7.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_8.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_9.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_10.json\n",
      " Sleeping 10.22s to avoid rate limit...\n",
      " Scraping r/Bitcoin...\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_11.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_12.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_13.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_14.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_15.json\n",
      " Sleeping 11.20s to avoid rate limit...\n",
      " Scraping r/BitcoinBeginners...\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_16.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_17.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_18.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_19.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_20.json\n",
      " Sleeping 9.58s to avoid rate limit...\n",
      " Scraping r/btc...\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_21.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_22.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_23.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_24.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_25.json\n",
      " Sleeping 10.66s to avoid rate limit...\n",
      " Scraping r/ethtrader...\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_26.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_27.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_28.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_29.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_30.json\n",
      " Sleeping 10.18s to avoid rate limit...\n",
      " Scraping r/ethereum...\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_31.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_32.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_33.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_34.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_35.json\n",
      " Sleeping 6.70s to avoid rate limit...\n",
      " Scraping r/XRP...\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_36.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_37.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_38.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_39.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_40.json\n",
      " Sleeping 7.20s to avoid rate limit...\n",
      " Scraping r/Ripple...\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_41.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_42.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_43.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_44.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_45.json\n",
      " Sleeping 11.13s to avoid rate limit...\n",
      " Scraping r/binance...\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_46.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_47.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_48.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_49.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_50.json\n",
      " Sleeping 7.45s to avoid rate limit...\n",
      " Scraping r/solana...\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_51.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_52.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_53.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_54.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_55.json\n",
      " Sleeping 9.01s to avoid rate limit...\n",
      " Scraping r/Tronix...\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_56.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_57.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_58.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_59.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_60.json\n",
      " Sleeping 11.83s to avoid rate limit...\n",
      " Scraping r/dogecoin...\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_61.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_62.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_63.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_64.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_65.json\n",
      " Sleeping 6.06s to avoid rate limit...\n",
      " Scraping r/cardano...\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_66.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_67.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_68.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_69.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_70.json\n",
      " Sleeping 6.02s to avoid rate limit...\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Function for Sentiment Analysis\n",
    "def get_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        return \"positive\"\n",
    "    elif polarity < -0.1:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "#Scrapper\n",
    "class RedditScraper:\n",
    "    def __init__(self, client_id, client_secret, user_agent, subreddits, post_limit=500, chunk_size=100, include_comments=True):\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "        self.subreddits = subreddits\n",
    "        self.post_limit = post_limit\n",
    "        self.chunk_size = chunk_size\n",
    "        self.include_comments = include_comments\n",
    "        self.output_dir = \"reddit_scrape_chunks\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.all_data = []\n",
    "        self.file_index = 1\n",
    "\n",
    "    def scrape(self):\n",
    "        for sub in self.subreddits:\n",
    "            print(f\" Scraping r/{sub}...\")\n",
    "            try:\n",
    "                for count, post in enumerate(self.reddit.subreddit(sub).top(limit=self.post_limit), 1):\n",
    "                    try:\n",
    "                        post_data = self._extract_post_data(post, sub)\n",
    "                        self.all_data.append(post_data)\n",
    "\n",
    "                        if count % self.chunk_size == 0:\n",
    "                            self._save_chunk(sub)\n",
    "                            self.all_data = []\n",
    "                            self.file_index += 1\n",
    "\n",
    "                        time.sleep(0.5)\n",
    "\n",
    "                    except Exception as post_err:\n",
    "                        print(f\" Error on post {post.id}: {post_err}\")\n",
    "                        time.sleep(2)\n",
    "\n",
    "                sleep_time = random.uniform(6, 12)\n",
    "                print(f\" Sleeping {sleep_time:.2f}s to avoid rate limit...\")\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Error in r/{sub}: {e}\")\n",
    "                if \"429\" in str(e):\n",
    "                    print(\" Hit rate limit. Sleeping for 60s...\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    time.sleep(5)\n",
    "\n",
    "        if self.all_data:\n",
    "            self._save_chunk(sub)\n",
    "\n",
    "    def _extract_post_data(self, post, subreddit):\n",
    "        post_data = {\n",
    "            'subreddit': subreddit,\n",
    "            'title': post.title,\n",
    "            'id': post.id,\n",
    "            'url': post.url,\n",
    "            'score': post.score,\n",
    "            'text': post.selftext,\n",
    "            'created_utc': post.created_utc,\n",
    "            'created_at': datetime.fromtimestamp(post.created_utc, timezone.utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'num_comments': post.num_comments,\n",
    "            'comments': []\n",
    "        }\n",
    "\n",
    "        if self.include_comments:\n",
    "            try:\n",
    "                post.comments.replace_more(limit=0)\n",
    "                post_data['comments'] = [{\n",
    "                    'id': comment.id,\n",
    "                    'author': str(comment.author),\n",
    "                    'body': comment.body,\n",
    "                    'score': comment.score,\n",
    "                    'created_utc': comment.created_utc,\n",
    "                    'created_at': datetime.fromtimestamp(comment.created_utc, timezone.utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'sentiment': get_sentiment(comment.body)\n",
    "                } for comment in post.comments.list()]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Error fetching comments for post {post.id}: {e}\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        return post_data\n",
    "\n",
    "    def _save_chunk(self, subreddit):\n",
    "        file_path = os.path.join(self.output_dir, f\"{subreddit}_batch_{self.file_index}.json\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.all_data, f, indent=2)\n",
    "        print(f\" Saved {len(self.all_data)} posts to {file_path}\")\n",
    "\n",
    "#Common points function \n",
    "class RedditCommentClusterer:\n",
    "    def __init__(self, json_folder='reddit_scrape_chunks', n_clusters=5):\n",
    "        self.json_folder = json_folder\n",
    "        self.n_clusters = n_clusters\n",
    "        self.comments = []\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.labels = []\n",
    "\n",
    "    def load_comments(self):\n",
    "        print(\"Loading comments...\")\n",
    "        for filename in os.listdir(self.json_folder):\n",
    "            if filename.endswith(\".json\"):\n",
    "                with open(os.path.join(self.json_folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                    posts = json.load(f)\n",
    "                    for post in posts:\n",
    "                        for comment in post.get(\"comments\", []):\n",
    "                            text = comment.get(\"body\", \"\").strip()\n",
    "                            if text:\n",
    "                                self.comments.append(text)\n",
    "        print(f\" Loaded {len(self.comments)} comments.\")\n",
    "\n",
    "    def vectorize_comments(self):\n",
    "        print(\" Vectorizing comments...\")\n",
    "        return self.vectorizer.fit_transform(self.comments)\n",
    "\n",
    "    def perform_clustering(self, X):\n",
    "        print(\" Performing KMeans clustering...\")\n",
    "        self.kmeans.fit(X)\n",
    "        self.labels = self.kmeans.labels_\n",
    "\n",
    "    def get_top_words_per_cluster(self, n_words=10):\n",
    "        print(\" Extracting top words per cluster...\")\n",
    "        terms = self.vectorizer.get_feature_names_out()\n",
    "        order_centroids = self.kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "        clusters = {}\n",
    "        for i in range(self.n_clusters):\n",
    "            top_words = [terms[ind] for ind in order_centroids[i, :n_words]]\n",
    "            clusters[i] = top_words\n",
    "        return clusters\n",
    "\n",
    "    def get_sample_comments_per_cluster(self, samples_per_cluster=3):\n",
    "        print(\" Collecting sample comments...\")\n",
    "        clustered_comments = {i: [] for i in range(self.n_clusters)}\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            if len(clustered_comments[label]) < samples_per_cluster:\n",
    "                clustered_comments[label].append(self.comments[idx])\n",
    "        return clustered_comments\n",
    "\n",
    "    def run(self):\n",
    "        self.load_comments()\n",
    "        X = self.vectorize_comments()\n",
    "        self.perform_clustering(X)\n",
    "        top_words = self.get_top_words_per_cluster()\n",
    "        sample_comments = self.get_sample_comments_per_cluster()\n",
    "\n",
    "        # Display results as DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            \"Top Words Per Cluster\": [', '.join(top_words[i]) for i in range(self.n_clusters)],\n",
    "            \"Sample Comments\": ['\\n\\n'.join(sample_comments[i]) for i in range(self.n_clusters)]\n",
    "        })\n",
    "        return df\n",
    "\n",
    "\n",
    "clusterer = RedditCommentClusterer(json_folder=\"reddit_scrape_chunks\", n_clusters=5)\n",
    "result_df = clusterer.run()\n",
    "\n",
    "# Display result \n",
    "import IPython.display as display\n",
    "display.display(result_df)\n",
    "\n",
    "\n",
    "\n",
    "scraper = RedditScraper(\n",
    "    client_id='94-KDboZSbfIo3SK3FDSzg',\n",
    "    client_secret='6ktVmT5--Uj7CeubVKIJJCII2tFsEQ',\n",
    "    user_agent='DataScrapper1811',\n",
    "    subreddits=[\n",
    "        'CryptoCurrency', 'CryptoMarkets',\n",
    "        'Bitcoin', 'BitcoinBeginners', 'btc',\n",
    "        'ethtrader', 'ethereum',\n",
    "        'XRP', 'Ripple',\n",
    "        'binance',\n",
    "        'solana',\n",
    "        'Tronix',\n",
    "        'dogecoin',\n",
    "        'cardano'\n",
    "    ],\n",
    "    post_limit=500,\n",
    "    chunk_size=100,\n",
    "    include_comments=True\n",
    ")\n",
    "\n",
    "scraper.scrape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0362e-05e4-4e43-a514-468a7dfb4d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
