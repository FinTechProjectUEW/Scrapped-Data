{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9818a63-136b-4a23-b95c-bf5d1a8c70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "#Giving the credentials using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id='94-KDboZSbfIo3SK3FDSzg',         \n",
    "    client_secret='6ktVmT5--Uj7CeubVKIJJCII2tFsEQ',\n",
    "    user_agent='DataScrapper1811'\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "subreddits = ['python', 'datascience', 'AskReddit']\n",
    "POST_LIMIT_PER_SUB = 500\n",
    "CHUNK_SIZE = 100\n",
    "INCLUDE_COMMENTS = True\n",
    "\n",
    "all_data = []\n",
    "file_index = 1\n",
    "\n",
    "# Creating an output folder\n",
    "output_dir = \"reddit_scrape_chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Beginning scraping\n",
    "for sub in subreddits:\n",
    "    print(f\" Scraping r/{sub}...\")\n",
    "\n",
    "    try:\n",
    "        for count, post in enumerate(reddit.subreddit(sub).top(limit=POST_LIMIT_PER_SUB), 1):\n",
    "            try:\n",
    "                post_data = {\n",
    "                    'subreddit': sub,\n",
    "                    'title': post.title,\n",
    "                    'id': post.id,\n",
    "                    'url': post.url,\n",
    "                    'score': post.score,\n",
    "                    'text': post.selftext,\n",
    "                    'created': post.created_utc,\n",
    "                    'num_comments': post.num_comments,\n",
    "                    'comments': []\n",
    "                }\n",
    "\n",
    "                if INCLUDE_COMMENTS:\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    post_data['comments'] = [{\n",
    "                        'id': comment.id,\n",
    "                        'author': str(comment.author),\n",
    "                        'body': comment.body,\n",
    "                        'score': comment.score,\n",
    "                        'created': comment.created_utc\n",
    "                    } for comment in post.comments.list()]\n",
    "\n",
    "                all_data.append(post_data)\n",
    "\n",
    "                # Saving in chunks\n",
    "                if count % CHUNK_SIZE == 0:\n",
    "                    file_path = os.path.join(output_dir, f\"{sub}_batch_{file_index}.json\")\n",
    "                    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(all_data, f, indent=2)\n",
    "                    print(f\" Saved {len(all_data)} posts to {file_path}\")\n",
    "                    all_data = []\n",
    "                    file_index += 1\n",
    "\n",
    "                time.sleep(0.5)  # slight delay per post\n",
    "\n",
    "            except Exception as post_err:\n",
    "                print(f\" Error on post {post.id}: {post_err}\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        # Pausing between subreddits\n",
    "        sleep_time = random.uniform(6, 12)\n",
    "        print(f\" Sleeping {sleep_time:.2f}s to avoid rate limit...\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error in r/{sub}: {e}\")\n",
    "        if \"429\" in str(e):\n",
    "            print(\" Hit rate limit. Sleeping for 60s...\")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            time.sleep(5)\n",
    "\n",
    "# Final saving if anything is left\n",
    "if all_data:\n",
    "    file_path = os.path.join(output_dir, f\"{sub}_batch_{file_index}.json\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "    print(f\" Final save: {len(all_data)} posts to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dd7d4e-c135-4538-b522-a1eb6194f655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scraping r/CryptoCurrency...\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_1.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_2.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_3.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_4.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoCurrency_batch_5.json\n",
      " Sleeping 11.16s to avoid rate limit...\n",
      " Scraping r/CryptoMarkets...\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_6.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_7.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_8.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_9.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\CryptoMarkets_batch_10.json\n",
      " Sleeping 11.67s to avoid rate limit...\n",
      " Scraping r/Bitcoin...\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_11.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_12.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_13.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_14.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Bitcoin_batch_15.json\n",
      " Sleeping 7.62s to avoid rate limit...\n",
      " Scraping r/BitcoinBeginners...\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_16.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_17.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_18.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_19.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\BitcoinBeginners_batch_20.json\n",
      " Sleeping 11.98s to avoid rate limit...\n",
      " Scraping r/btc...\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_21.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_22.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_23.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_24.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\btc_batch_25.json\n",
      " Sleeping 11.88s to avoid rate limit...\n",
      " Scraping r/ethtrader...\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_26.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_27.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_28.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_29.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethtrader_batch_30.json\n",
      " Sleeping 7.01s to avoid rate limit...\n",
      " Scraping r/ethereum...\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_31.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_32.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_33.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_34.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\ethereum_batch_35.json\n",
      " Sleeping 7.42s to avoid rate limit...\n",
      " Scraping r/XRP...\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_36.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_37.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_38.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_39.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\XRP_batch_40.json\n",
      " Sleeping 10.29s to avoid rate limit...\n",
      " Scraping r/Ripple...\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_41.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_42.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_43.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_44.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Ripple_batch_45.json\n",
      " Sleeping 9.22s to avoid rate limit...\n",
      " Scraping r/binance...\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_46.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_47.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_48.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_49.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\binance_batch_50.json\n",
      " Sleeping 9.30s to avoid rate limit...\n",
      " Scraping r/solana...\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_51.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_52.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_53.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_54.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\solana_batch_55.json\n",
      " Sleeping 7.26s to avoid rate limit...\n",
      " Scraping r/Tronix...\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_56.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_57.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_58.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_59.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\Tronix_batch_60.json\n",
      " Sleeping 6.30s to avoid rate limit...\n",
      " Scraping r/dogecoin...\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_61.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_62.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_63.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_64.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\dogecoin_batch_65.json\n",
      " Sleeping 10.59s to avoid rate limit...\n",
      " Scraping r/cardano...\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_66.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_67.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_68.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_69.json\n",
      " Saved 100 posts to reddit_scrape_chunks\\cardano_batch_70.json\n",
      " Sleeping 6.96s to avoid rate limit...\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "#Giving the credentials using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id='94-KDboZSbfIo3SK3FDSzg',         \n",
    "    client_secret='6ktVmT5--Uj7CeubVKIJJCII2tFsEQ',\n",
    "    user_agent='DataScrapper1811'\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "subreddits = [\n",
    "    'CryptoCurrency', 'CryptoMarkets',\n",
    "    'Bitcoin', 'BitcoinBeginners', 'btc',\n",
    "    'ethtrader', 'ethereum',\n",
    "    'XRP', 'Ripple',\n",
    "    'binance',\n",
    "    'solana',\n",
    "    'Tronix',\n",
    "    'dogecoin',\n",
    "    'cardano'\n",
    "]\n",
    "POST_LIMIT_PER_SUB = 500\n",
    "CHUNK_SIZE = 100\n",
    "INCLUDE_COMMENTS = True\n",
    "\n",
    "all_data = []\n",
    "file_index = 1\n",
    "\n",
    "# Creating an output folder\n",
    "output_dir = \"reddit_scrape_chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Beginning scraping\n",
    "for sub in subreddits:\n",
    "    print(f\" Scraping r/{sub}...\")\n",
    "\n",
    "    try:\n",
    "        for count, post in enumerate(reddit.subreddit(sub).top(limit=POST_LIMIT_PER_SUB), 1):\n",
    "            try:\n",
    "                post_data = {\n",
    "                    'subreddit': sub,\n",
    "                    'title': post.title,\n",
    "                    'id': post.id,\n",
    "                    'url': post.url,\n",
    "                    'score': post.score,\n",
    "                    'text': post.selftext,\n",
    "                    'created': post.created_utc,\n",
    "                    'num_comments': post.num_comments,\n",
    "                    'comments': []\n",
    "                }\n",
    "\n",
    "                if INCLUDE_COMMENTS:\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    post_data['comments'] = [{\n",
    "                        'id': comment.id,\n",
    "                        'author': str(comment.author),\n",
    "                        'body': comment.body,\n",
    "                        'score': comment.score,\n",
    "                        'created': comment.created_utc\n",
    "                    } for comment in post.comments.list()]\n",
    "\n",
    "                all_data.append(post_data)\n",
    "\n",
    "                # Saving in chunks\n",
    "                if count % CHUNK_SIZE == 0:\n",
    "                    file_path = os.path.join(output_dir, f\"{sub}_batch_{file_index}.json\")\n",
    "                    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(all_data, f, indent=2)\n",
    "                    print(f\" Saved {len(all_data)} posts to {file_path}\")\n",
    "                    all_data = []\n",
    "                    file_index += 1\n",
    "\n",
    "                time.sleep(0.5)  # slight delay per post\n",
    "\n",
    "            except Exception as post_err:\n",
    "                print(f\" Error on post {post.id}: {post_err}\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        # Pausing between subreddits\n",
    "        sleep_time = random.uniform(6, 12)\n",
    "        print(f\" Sleeping {sleep_time:.2f}s to avoid rate limit...\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error in r/{sub}: {e}\")\n",
    "        if \"429\" in str(e):\n",
    "            print(\" Hit rate limit. Sleeping for 60s...\")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            time.sleep(5)\n",
    "\n",
    "# Final saving if anything is left\n",
    "if all_data:\n",
    "    file_path = os.path.join(output_dir, f\"{sub}_batch_{file_index}.json\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "    print(f\" Final save: {len(all_data)} posts to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc38d2-70ca-498a-b93b-95ad19280301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
